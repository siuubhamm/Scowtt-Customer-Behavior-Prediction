{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425cdbdc",
   "metadata": {},
   "source": [
    "## Training models for Customer Product Purchase Propensity and Purchase Order Value prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12243a2e",
   "metadata": {},
   "source": [
    "### Customer Purchase Propensity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdc3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import average_precision_score, mean_absolute_error, auc, precision_recall_curve, roc_curve, mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14442c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"tot_pymt_sqntl\",\n",
    "    \"avg_pymt_instllmnt\",\n",
    "    \"tot_pymt_val\",\n",
    "    \"tot_pymt_boleto\",\n",
    "    \"tot_pymt_debit_card\",\n",
    "    \"tot_pymt_not_defined\",\n",
    "    \"tot_pymt_voucher\",\n",
    "    \"avg_rev_score\",\n",
    "    \"avg_rev_title_length\",\n",
    "    \"avg_rev_length\",\n",
    "    \"days_since_lst_rev_creation\",\n",
    "    \"num_products\",\n",
    "    \"avg_order_size\",\n",
    "    \"tot_order_freight_value\",\n",
    "    \"num_orders_approved\",\n",
    "    \"num_orders_canceled\",\n",
    "    \"num_orders_created\",\n",
    "    \"num_orders_invoiced\",\n",
    "    \"num_orders_processing\",\n",
    "    \"num_orders_shipped\",\n",
    "    \"num_orders_unavailable\",\n",
    "    \"tot_pymt_val_canceled\",\n",
    "    \"tot_pymt_val_invoiced\",\n",
    "    \"tot_pymt_val_processing\",\n",
    "    \"tot_pymt_val_shipped\",\n",
    "    \"tot_pymt_val_unavailable\",\n",
    "    \"num_rev_delivered\",\n",
    "    \"num_products_created\",\n",
    "    \"num_products_unavailable\",\n",
    "    \"avg_order_size_created\",\n",
    "    \"avg_order_size_unavailable\",\n",
    "    \"tot_order_price_created\",\n",
    "    \"tot_order_price_unavailable\",\n",
    "    \"tot_order_freight_value_canceled\",\n",
    "    \"tot_order_freight_value_created\",\n",
    "    \"tot_order_value_created\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8687f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNAPSHOT_MONTH = \"06-2018\"\n",
    "SNAPSHOT_PATH = f\"../../Data/processed/customer_snapshots/{SNAPSHOT_MONTH}/customer_unique_snapshot.csv\"\n",
    "ORDERS_PATH = \"../../Data/raw/olist_orders_dataset.csv\"\n",
    "INTERMEDIATE_ORDERS_PATH = \"../../Data/processed/intermediate_output_orders.csv\"\n",
    "CUSTOMERS_PATH = \"../../Data/raw/olist_customers_dataset.csv\"\n",
    "\n",
    "CUTOFF_DATE = pd.Timestamp(\"2018-06-30\")\n",
    "HORIZON_DAYS = 90\n",
    "HORIZON_END = CUTOFF_DATE + pd.Timedelta(days=HORIZON_DAYS)\n",
    "RECENCY_THRESHOLD = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6674179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = pd.read_csv(SNAPSHOT_PATH).set_index(\"customer_unique_id\")\n",
    "\n",
    "X_filtered = X_raw[\n",
    "    X_raw[\"days_since_lst_order_purchased\"] <= RECENCY_THRESHOLD\n",
    "].copy()\n",
    "\n",
    "orders = pd.read_csv(\n",
    "    ORDERS_PATH,\n",
    "    parse_dates=[\"order_purchase_timestamp\"]\n",
    ")\n",
    "\n",
    "order_values = pd.read_csv(\n",
    "    INTERMEDIATE_ORDERS_PATH\n",
    ")[[\"order_id\", \"tot_order_value\"]]\n",
    "\n",
    "customers = pd.read_csv(\n",
    "    CUSTOMERS_PATH\n",
    ")[[\"customer_id\", \"customer_unique_id\"]]\n",
    "\n",
    "orders = orders.merge(order_values, on=\"order_id\", how=\"inner\")\n",
    "\n",
    "future_orders = orders[\n",
    "    (orders[\"order_purchase_timestamp\"] > CUTOFF_DATE) &\n",
    "    (orders[\"order_purchase_timestamp\"] <= HORIZON_END)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591447a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = future_orders.merge(customers, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "labels = labels.groupby(\"customer_unique_id\").agg(\n",
    "    y_propensity=(\"order_id\", lambda x: 1),\n",
    "    y_value=(\"tot_order_value\", \"sum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5238cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_filtered.join(labels, how=\"left\")\n",
    "\n",
    "data[\"y_propensity\"] = data[\"y_propensity\"].fillna(0).astype(int)\n",
    "data[\"y_value\"] = data[\"y_value\"].fillna(0.0)\n",
    "\n",
    "final_feature_list = [\n",
    "    f for f in FEATURES if f in data.columns\n",
    "]\n",
    "\n",
    "X_model = data[final_feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb6f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, yv_train, yv_val = train_test_split(\n",
    "    X_model,\n",
    "    data[\"y_propensity\"],\n",
    "    data[\"y_value\"],\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=data[\"y_propensity\"]   \n",
    ")\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val = pd.DataFrame(\n",
    "    X_val_scaled,\n",
    "    columns=X_val.columns,\n",
    "    index=X_val.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03007ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune(model_class, param_grid, X_train, y_train, X_val, y_val, fixed_params=None):\n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    fixed_params = fixed_params or {}\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        model = model_class(**fixed_params, **params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        score = average_precision_score(y_val, preds)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b21ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression ===\n",
      "Best Params: {'C': 0.1, 'class_weight': None, 'penalty': 'l1'}\n",
      "Val PR-AUC: 0.0135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1.0],\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "lr_fixed = {\n",
    "    \"solver\": \"liblinear\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "lr_model, lr_params, lr_score = train_and_tune(\n",
    "    LogisticRegression,\n",
    "    lr_param_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=lr_fixed\n",
    ")\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(\"Best Params:\", lr_params)\n",
    "print(f\"Val PR-AUC: {lr_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a957eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest ===\n",
      "Best Params: {'max_depth': 5, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Val PR-AUC: 0.0182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [None, 3, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 5]\n",
    "}\n",
    "\n",
    "rf_fixed = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "rf_model, rf_params, rf_score = train_and_tune(\n",
    "    RandomForestClassifier,\n",
    "    rf_param_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=rf_fixed\n",
    ")\n",
    "\n",
    "print(\"=== Random Forest ===\")\n",
    "print(\"Best Params:\", rf_params)\n",
    "print(f\"Val PR-AUC: {rf_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b08e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost ===\n",
      "Best Params: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Val PR-AUC: 0.0684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_param_grid = {\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "\n",
    "xgb_fixed = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"scale_pos_weight\": pos_weight,\n",
    "    \"use_label_encoder\": False,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "xgb_model, xgb_params, xgb_score = train_and_tune(\n",
    "    XGBClassifier,\n",
    "    xgb_param_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=xgb_fixed\n",
    ")\n",
    "\n",
    "print(\"=== XGBoost ===\")\n",
    "print(\"Best Params:\", xgb_params)\n",
    "print(f\"Val PR-AUC: {xgb_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b13233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepurchaseMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_on_quarter(\n",
    "    snapshot_month,\n",
    "    model=None,\n",
    "    scaler=None,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    val_frac=0.2\n",
    "):\n",
    "    SNAPSHOT_PATH = f\"../../Data/processed/customer_snapshots/{snapshot_month}/customer_unique_snapshot.csv\"\n",
    "\n",
    "    df = pd.read_csv(SNAPSHOT_PATH).set_index(\"customer_unique_id\")\n",
    "    df = df[df[\"days_since_lst_order_purchased\"] <= RECENCY_THRESHOLD].copy()\n",
    "\n",
    "    df = df.join(labels, how=\"left\")\n",
    "    df[\"y_propensity\"] = df[\"y_propensity\"].fillna(0).astype(int)\n",
    "\n",
    "    final_features = [f for f in FEATURES if f in df.columns]\n",
    "\n",
    "    X_raw = df[final_features].fillna(0).values\n",
    "    y_raw = df[\"y_propensity\"].values\n",
    "\n",
    "    X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "        X_raw,\n",
    "        y_raw,\n",
    "        test_size=val_frac,\n",
    "        random_state=42,\n",
    "        stratify=y_raw\n",
    "    )\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train_raw)\n",
    "    else:\n",
    "        X_train = scaler.transform(X_train_raw)\n",
    "\n",
    "    X_val = scaler.transform(X_val_raw)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    class_counts = np.bincount(y_train.astype(int))\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = class_weights[y_train.astype(int)]\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    if model is None:\n",
    "        model = RepurchaseMLP(X_train.shape[1])\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"\\n--- Training | Snapshot {snapshot_month} ---\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_probs = model(torch.FloatTensor(X_train)).numpy().ravel()\n",
    "                val_probs = model(torch.FloatTensor(X_val)).numpy().ravel()\n",
    "\n",
    "                train_pr = average_precision_score(y_train, train_probs)\n",
    "                val_pr = average_precision_score(y_val, val_probs)\n",
    "\n",
    "                train_roc = roc_auc_score(y_train, train_probs)\n",
    "                val_roc = roc_auc_score(y_val, val_probs)\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {epoch:03d} | \"\n",
    "                    f\"Loss: {epoch_loss/len(train_loader):.4f} | \"\n",
    "                    f\"Train PR: {train_pr:.4f} | Val PR: {val_pr:.4f} | \"\n",
    "                    f\"Train ROC: {train_roc:.4f} | Val ROC: {val_roc:.4f}\"\n",
    "                )\n",
    "\n",
    "    return model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d56d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training | Snapshot 06-2018 ---\n",
      "Epoch 001 | Loss: 0.6665 | Train PR: 0.0316 | Val PR: 0.0105 | Train ROC: 0.7167 | Val ROC: 0.5823\n",
      "Epoch 010 | Loss: 0.4488 | Train PR: 0.1215 | Val PR: 0.0089 | Train ROC: 0.9131 | Val ROC: 0.5050\n",
      "Epoch 020 | Loss: 0.3380 | Train PR: 0.2804 | Val PR: 0.0125 | Train ROC: 0.9595 | Val ROC: 0.5243\n",
      "Epoch 030 | Loss: 0.2992 | Train PR: 0.3660 | Val PR: 0.0088 | Train ROC: 0.9752 | Val ROC: 0.5360\n",
      "Epoch 040 | Loss: 0.2733 | Train PR: 0.3756 | Val PR: 0.0090 | Train ROC: 0.9793 | Val ROC: 0.5418\n",
      "Epoch 050 | Loss: 0.2641 | Train PR: 0.4764 | Val PR: 0.0078 | Train ROC: 0.9833 | Val ROC: 0.5357\n",
      "Epoch 060 | Loss: 0.2525 | Train PR: 0.4341 | Val PR: 0.0084 | Train ROC: 0.9818 | Val ROC: 0.5382\n",
      "Epoch 070 | Loss: 0.2489 | Train PR: 0.4560 | Val PR: 0.0075 | Train ROC: 0.9861 | Val ROC: 0.5531\n",
      "Epoch 080 | Loss: 0.2372 | Train PR: 0.4595 | Val PR: 0.0072 | Train ROC: 0.9875 | Val ROC: 0.5374\n",
      "Epoch 090 | Loss: 0.2379 | Train PR: 0.5245 | Val PR: 0.0072 | Train ROC: 0.9903 | Val ROC: 0.5382\n",
      "Epoch 100 | Loss: 0.2294 | Train PR: 0.5315 | Val PR: 0.0077 | Train ROC: 0.9904 | Val ROC: 0.5493\n"
     ]
    }
   ],
   "source": [
    "quarters = [\"06-2018\"]\n",
    "\n",
    "current_model = None\n",
    "current_scaler = None\n",
    "\n",
    "for q in quarters:\n",
    "    current_model, current_scaler = train_on_quarter(\n",
    "        q,\n",
    "        model=current_model,\n",
    "        scaler=current_scaler,\n",
    "        epochs=100\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        current_model.state_dict(),\n",
    "        f\"repurchase_mlp_progressive_{q}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ca13c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC curve saved to: ../../Reports/pr_auc_curves.png\n"
     ]
    }
   ],
   "source": [
    "def get_probs(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def plot_pr_curves(models, X, y, save_path=\"../../Reports/pr_auc_curves.png\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for name, model in models.items():\n",
    "        probs = model.predict_proba(X)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y, probs)\n",
    "        pr_auc = average_precision_score(y, probs)\n",
    "\n",
    "        plt.plot(recall, precision, label=f\"{name} (PR-AUC={pr_auc:.3f})\")\n",
    "\n",
    "    baseline = y.mean()\n",
    "    plt.hlines(\n",
    "        baseline,\n",
    "        xmin=0,\n",
    "        xmax=1,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"dashed\",\n",
    "        label=f\"Baseline={baseline:.3f}\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision–Recall Curve – Conversion Propensity Models\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"PR-AUC curve saved to: {save_path}\")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "}\n",
    "\n",
    "plot_pr_curves(models, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04d21891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        feature  importance\n",
      "0                tot_pymt_sqntl    0.107190\n",
      "11                 num_products    0.097536\n",
      "26            num_rev_delivered    0.077211\n",
      "3               tot_pymt_boleto    0.075451\n",
      "13      tot_order_freight_value    0.074841\n",
      "2                  tot_pymt_val    0.073024\n",
      "8          avg_rev_title_length    0.068613\n",
      "7                 avg_rev_score    0.065753\n",
      "1            avg_pymt_instllmnt    0.065687\n",
      "9                avg_rev_length    0.064052\n",
      "10  days_since_lst_rev_creation    0.061463\n",
      "12               avg_order_size    0.059916\n",
      "21        tot_pymt_val_canceled    0.056985\n",
      "6              tot_pymt_voucher    0.052279\n",
      "5          tot_pymt_not_defined    0.000000\n",
      "4           tot_pymt_debit_card    0.000000\n",
      "16           num_orders_created    0.000000\n",
      "17          num_orders_invoiced    0.000000\n",
      "14          num_orders_approved    0.000000\n",
      "15          num_orders_canceled    0.000000\n"
     ]
    }
   ],
   "source": [
    "xgb_importance = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(xgb_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d193f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/Purchase Propensity/xgb_model_model_06_2018.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_model, \"../../Models/Purchase Propensity/xgb_model_model_06_2018.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206ae73",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "- Class Imbalance: the number of people who go for repeat purchases are very low(~3%), indicating that there is an imbalance in the target variable.\n",
    "\n",
    "- Logistic Regression performs poorly indicating a linear is not enough.\n",
    "- Random Forest gives slight improvement over LR.\n",
    "- XGBoost gives the best results on the validation set.\n",
    "- MLP overfits the training data and does not generalize well on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a379f92",
   "metadata": {},
   "source": [
    "### Predicting Customer Purchase Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5de6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Data/processed/customer_snapshots/10-2018/customer_unique_snapshot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6a0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DAYS = 90\n",
    "TARGET = \"avg_order_value\"\n",
    "\n",
    "df_value = df[df[\"num_orders\"] > 1].copy()\n",
    "df_value = df_value.dropna(subset = [TARGET])\n",
    "\n",
    "cat_features = [\n",
    "    \"customer_city\",\n",
    "    \"customer_state\",\n",
    "    \"pref_prod_category\",\n",
    "    \"pref_prod_category_english\",\n",
    "]\n",
    "\n",
    "for col in cat_features:\n",
    "    df_value[col] = df_value[col].astype(\"category\")\n",
    "\n",
    "df_value[\"y\"] = np.log1p(df_value[TARGET])\n",
    "\n",
    "train_mask = df_value[\"days_since_lst_order_purchased\"] > CUTOFF_DAYS\n",
    "val_mask   = df_value[\"days_since_lst_order_purchased\"] <= CUTOFF_DAYS\n",
    "\n",
    "X_train = df_value.loc[train_mask, FEATURES]\n",
    "y_train = df_value.loc[train_mask, \"y\"]\n",
    "\n",
    "X_val = df_value.loc[val_mask, FEATURES]\n",
    "y_val = df_value.loc[val_mask, \"y\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    X_val_scaled,\n",
    "    columns=X_val.columns,\n",
    "    index=X_val.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5c02bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in FEATURES:\n",
    "    if X_train_scaled[col].isna().all():\n",
    "        X_train_scaled[col] = 0.0\n",
    "        X_val_scaled[col] = 0.0\n",
    "    else:\n",
    "        med = X_train_scaled[col].median()\n",
    "        X_train_scaled[col] = X_train_scaled[col].fillna(med)\n",
    "        X_val_scaled[col] = X_val_scaled[col].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "809249d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X_train_scaled.isna().any().any()\n",
    "assert not X_val_scaled.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune_regressor(\n",
    "    model_class,\n",
    "    param_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=None\n",
    "):\n",
    "    best_mae = np.inf\n",
    "    best_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    fixed_params = fixed_params or {}\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        model = model_class(**fixed_params, **params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(np.expm1(y_val), np.expm1(preds))\n",
    "        rmse = np.sqrt(mean_squared_error(np.expm1(y_val), np.expm1(preds)))\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "    return best_model, best_params, best_mae, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81edc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ridge Regression ===\n",
      "Best Params: {'alpha': 0.1}\n",
      "MAE: 41789.544 | RMSE: 620875.017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ridge_grid = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "ridge_model, ridge_params, ridge_mae, ridge_rmse = train_and_tune_regressor(\n",
    "    Ridge,\n",
    "    ridge_grid,\n",
    "    X_train_scaled, y_train,\n",
    "    X_val_scaled, y_val\n",
    ")\n",
    "\n",
    "print(\"=== Ridge Regression ===\")\n",
    "print(\"Best Params:\", ridge_params)\n",
    "print(f\"MAE: {ridge_mae:.3f} | RMSE: {ridge_rmse:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72799e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Regressor ===\n",
      "Best Params: {'max_depth': None, 'max_features': 0.7, 'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "MAE: 16.903 | RMSE: 142.445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [300, 600],\n",
    "    \"max_depth\": [None, 8, 12],\n",
    "    \"min_samples_leaf\": [5, 10],\n",
    "    \"max_features\": [\"sqrt\", 0.7]\n",
    "}\n",
    "\n",
    "rf_fixed = {\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "rf_model, rf_params, rf_mae, rf_rmse = train_and_tune_regressor(\n",
    "    RandomForestRegressor,\n",
    "    rf_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=rf_fixed\n",
    ")\n",
    "\n",
    "print(\"=== Random Forest Regressor ===\")\n",
    "print(\"Best Params:\", rf_params)\n",
    "print(f\"MAE: {rf_mae:.3f} | RMSE: {rf_rmse:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "106367f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Regressor ===\n",
      "Best Params: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.8}\n",
      "MAE: 23.034 | RMSE: 173.334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_grid = {\n",
    "    \"max_depth\": [5, 8],\n",
    "    \"learning_rate\": [0.03, 0.05],\n",
    "    \"n_estimators\": [400, 800],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8]\n",
    "}\n",
    "\n",
    "xgb_fixed = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"min_child_weight\": 50,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "xgb_model, xgb_params, xgb_mae, xgb_rmse = train_and_tune_regressor(\n",
    "    XGBRegressor,\n",
    "    xgb_grid,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    fixed_params=xgb_fixed\n",
    ")\n",
    "\n",
    "print(\"=== XGBoost Regressor ===\")\n",
    "print(\"Best Params:\", xgb_params)\n",
    "print(f\"MAE: {xgb_mae:.3f} | RMSE: {xgb_rmse:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a41af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              true        pred\n",
      "decile                        \n",
      "0        38.386739   37.667618\n",
      "1        60.163333   58.786598\n",
      "2        73.706970   72.754797\n",
      "3        87.740217   89.680321\n",
      "4       107.408442  105.657411\n",
      "5       118.249508  117.400955\n",
      "6       136.867993  140.724182\n",
      "7       165.232891  167.704566\n",
      "8       211.927283  212.092710\n",
      "9       570.191630  450.491365\n"
     ]
    }
   ],
   "source": [
    "log_preds = rf_model.predict(X_val)\n",
    "preds = np.expm1(log_preds)\n",
    "true_vals = np.expm1(y_val)\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"true\": true_vals,\n",
    "    \"pred\": preds\n",
    "})\n",
    "\n",
    "eval_df[\"decile\"] = pd.qcut(\n",
    "    eval_df[\"pred\"],\n",
    "    q=10,\n",
    "    labels=False,\n",
    "    duplicates=\"drop\"\n",
    ")\n",
    "\n",
    "decile_summary = eval_df.groupby(\"decile\")[[\"true\", \"pred\"]].mean()\n",
    "\n",
    "print(decile_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3196c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             feature  importance\n",
      "2                       tot_pymt_val    0.854516\n",
      "11                      num_products    0.055740\n",
      "13           tot_order_freight_value    0.054452\n",
      "3                    tot_pymt_boleto    0.015547\n",
      "1                 avg_pymt_instllmnt    0.008256\n",
      "26                 num_rev_delivered    0.003591\n",
      "10       days_since_lst_rev_creation    0.002462\n",
      "0                     tot_pymt_sqntl    0.002027\n",
      "9                     avg_rev_length    0.000956\n",
      "12                    avg_order_size    0.000703\n",
      "7                      avg_rev_score    0.000646\n",
      "25          tot_pymt_val_unavailable    0.000532\n",
      "21             tot_pymt_val_canceled    0.000204\n",
      "6                   tot_pymt_voucher    0.000151\n",
      "8               avg_rev_title_length    0.000104\n",
      "20            num_orders_unavailable    0.000062\n",
      "33  tot_order_freight_value_canceled    0.000032\n",
      "15               num_orders_canceled    0.000014\n",
      "24              tot_pymt_val_shipped    0.000004\n",
      "4                tot_pymt_debit_card    0.000003\n"
     ]
    }
   ],
   "source": [
    "rf_importance = pd.DataFrame({\n",
    "    \"feature\": X_train_scaled.columns,\n",
    "    \"importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(rf_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea5cc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/Order Value/rf_value_model_06_2018.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_model, \"../../Models/Order Value/rf_value_model_06_2018.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba2d3f",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "- Linear Regression performs very poorly, showing that a linear model in itself is not enough to capture the customer behavior.\n",
    "- Random Forest and XGBoost Regressor fit the training data in a much better way.\n",
    "- The best results are from Random Forest Regressor\n",
    "- Decile level scores suggest that the model is able to closely predict the order values for top o9 deciles. There is some deviation in the 10th decile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
