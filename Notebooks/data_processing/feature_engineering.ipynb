{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb56c75",
   "metadata": {},
   "source": [
    "## Building features to prepare the Customer Master Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88eeec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2dd72",
   "metadata": {},
   "source": [
    "### Checking Overlaps between different tables before joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe51f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Category Name coverage: 0.98\n",
      "Goelocations Zip Code coverage: 0.9977382875605816\n"
     ]
    }
   ],
   "source": [
    "# product_translations will join with olist_product and geolocation will join with seller data.\n",
    "\n",
    "product_translations = pd.read_csv(\"../../Data/raw/product_category_name_translation.csv\")\n",
    "products = pd.read_csv(\"../../Data/raw/olist_products_dataset.csv\")\n",
    "\n",
    "geolocations = pd.read_csv(\"../../Data/raw/olist_geolocation_dataset.csv\")\n",
    "sellers = pd.read_csv(\"../../Data/raw/olist_sellers_dataset.csv\")\n",
    "\n",
    "#Overlap between product translations and products\n",
    "prod_category_match_prop = products['product_category_name'].isin(product_translations['product_category_name']).mean()\n",
    "print(f\"Product Category Name coverage: {prod_category_match_prop:.2f}\")\n",
    "\n",
    "#Overlap between sellers and geolocations\n",
    "geo_match_prop = sellers[\"seller_zip_code_prefix\"].isin(geolocations[\"geolocation_zip_code_prefix\"]).mean()\n",
    "print(f\"Goelocations Zip Code coverage: {geo_match_prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15a973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID coverage: 1.00\n",
      "Seller ID coverage:  1.00\n"
     ]
    }
   ],
   "source": [
    "# olist_products and olist_sellers will join with olist_order_items dataset.\n",
    "\n",
    "order_items = pd.read_csv(\"../../Data/raw/olist_order_items_dataset.csv\")\n",
    "\n",
    "# Overlap between products and orders\n",
    "prod_match_prop = order_items['product_id'].isin(products['product_id']).mean()\n",
    "\n",
    "# Overlap between ssellers and orders\n",
    "seller_match_prop = order_items['seller_id'].isin(sellers['seller_id']).mean()\n",
    "\n",
    "print(f\"Product ID coverage: {prod_match_prop:.2f}\")\n",
    "print(f\"Seller ID coverage:  {seller_match_prop:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050aeb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review ID coverage: 0.99\n",
      "Payment ID coverage:  1.00\n"
     ]
    }
   ],
   "source": [
    "# reviews, payments and order items will join on orders data.\n",
    "\n",
    "reviews = pd.read_csv(\"../../Data/raw/olist_order_reviews_dataset.csv\", parse_dates=[\"review_creation_date\"])\n",
    "payments = pd.read_csv(\"../../Data/raw/olist_order_payments_dataset.csv\")\n",
    "orders = pd.read_csv(\"../../Data/raw/olist_orders_dataset.csv\", parse_dates=[\"order_purchase_timestamp\"])\n",
    "\n",
    "rev_match_prop = orders[\"order_id\"].isin(reviews[\"order_id\"]).mean()\n",
    "payment_match_prop = orders[\"order_id\"].isin(payments[\"order_id\"]).mean()\n",
    "\n",
    "print(f\"Review ID coverage: {rev_match_prop:.2f}\")\n",
    "print(f\"Payment ID coverage:  {payment_match_prop:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f04c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID coverage: 1.00\n"
     ]
    }
   ],
   "source": [
    "# orders data will join on customers data.\n",
    "\n",
    "customers = pd.read_csv(\"../../Data/raw/olist_customers_dataset.csv\")\n",
    "\n",
    "order_match_prop = customers[\"customer_id\"].isin(orders[\"customer_id\"]).mean()\n",
    "\n",
    "print(f\"Customer ID coverage: {order_match_prop:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb100377",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "- 98% of the product categories have an english translation.\n",
    "- 99% of the orders have an entry in the reviews dataset: which is natural as not all the orders are expected to be having a review.\n",
    "- 99.7% of the sellers geolocations are not present in the geolocations dataset.\n",
    "- Other than the above tables, rest of the tables have a perfect match for order_id and customer_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03cead",
   "metadata": {},
   "source": [
    "### Joining the tables to prepare the master table.\n",
    "\n",
    "1. Preparing the order level details- this will act as the intermediate output.\n",
    "2. This output will then be joined with customer dataset and aggregated from order_id to customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5273a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mode(series):\n",
    "    m = series.mode(dropna=True)\n",
    "    return m.iloc[0] if not m.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b90cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_agg_df = (\n",
    "    geolocations\n",
    "    .groupby(\"geolocation_zip_code_prefix\", as_index=False)\n",
    "    .agg(\n",
    "        geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "        geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "        geolocation_city=(\"geolocation_city\", \"first\"),\n",
    "        geolocation_state=(\"geolocation_state\", \"first\")\n",
    "    )\n",
    ")\n",
    "\n",
    "products_enriched_df = (\n",
    "    products\n",
    "    .merge(\n",
    "        product_translations,\n",
    "        on=\"product_category_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sellers_enriched_df = (\n",
    "    sellers\n",
    "    .merge(\n",
    "        geo_agg_df,\n",
    "        left_on=\"seller_zip_code_prefix\",\n",
    "        right_on=\"geolocation_zip_code_prefix\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d117ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_products_df = (\n",
    "    order_items\n",
    "    .merge(\n",
    "        products_enriched_df,\n",
    "        on=\"product_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "order_items_super_df = (\n",
    "    order_items_products_df\n",
    "    .merge(\n",
    "        sellers_enriched_df,\n",
    "        on=\"seller_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "order_items_with_ts_df = (\n",
    "    order_items_super_df\n",
    "    .merge(\n",
    "        orders[[\"order_id\", \"order_purchase_timestamp\"]],\n",
    "        on=\"order_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# calculating the total order value.\n",
    "order_items_with_ts_df[\"total_value\"] = order_items_with_ts_df[\"price\"] + order_items_with_ts_df[\"freight_value\"]\n",
    "\n",
    "order_items_with_ts_df[\"shipping_limit_date\"] = pd.to_datetime(order_items_with_ts_df[\"shipping_limit_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9177b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating order items dataset at order_id, product_id and seller_id level - as that is the granularity of this dataset.\n",
    "\n",
    "order_product_seller_agg_df = (\n",
    "    order_items_with_ts_df\n",
    "    .groupby(\n",
    "        [\"order_id\", \"product_id\", \"seller_id\"],\n",
    "        as_index=False\n",
    "    )\n",
    "    .agg(\n",
    "        # timestamps\n",
    "        order_purchase_timestamp=(\"order_purchase_timestamp\", \"first\"),\n",
    "        last_shipping_limit_date=(\"shipping_limit_date\", \"max\"),\n",
    "\n",
    "        # item count\n",
    "        order_item_cnt=(\"order_item_id\", \"nunique\"),\n",
    "\n",
    "        # monetary\n",
    "        price=(\"price\", \"sum\"),\n",
    "        freight_value=(\"freight_value\", \"sum\"),\n",
    "        total_value=(\"total_value\", \"sum\"),\n",
    "\n",
    "        # product attributes\n",
    "        product_category_name=(\"product_category_name\", \"first\"),\n",
    "        product_name_lenght=(\"product_name_lenght\", \"first\"),\n",
    "        product_description_lenght=(\"product_description_lenght\", \"first\"),\n",
    "        product_photos_qty=(\"product_photos_qty\", \"first\"),\n",
    "        product_weight_g=(\"product_weight_g\", \"first\"),\n",
    "        product_length_cm=(\"product_length_cm\", \"first\"),\n",
    "        product_height_cm=(\"product_height_cm\", \"first\"),\n",
    "        product_width_cm=(\"product_width_cm\", \"first\"),\n",
    "        product_category_name_english=(\"product_category_name_english\", \"first\"),\n",
    "\n",
    "        # seller attributes\n",
    "        seller_zip_code_prefix=(\"seller_zip_code_prefix\", \"first\"),\n",
    "        seller_city=(\"seller_city\", \"first\"),\n",
    "        seller_state=(\"seller_state\", \"first\"),\n",
    "        geolocation_lat=(\"geolocation_lat\", \"first\"),\n",
    "        geolocation_lng=(\"geolocation_lng\", \"first\"),\n",
    "        geolocation_city=(\"geolocation_city\", \"first\"),\n",
    "        geolocation_state=(\"geolocation_state\", \"first\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057bdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the feature days since last shipped\n",
    "\n",
    "order_product_seller_agg_df[\"days_since_last_shipped\"] = (\n",
    "    order_product_seller_agg_df[\"order_purchase_timestamp\"]\n",
    "    - order_product_seller_agg_df[\"last_shipping_limit_date\"]\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abc6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "# Uniqueness at target grain\n",
    "assert order_product_seller_agg_df.duplicated(\n",
    "    [\"order_id\", \"product_id\", \"seller_id\"]\n",
    ").sum() == 0\n",
    "\n",
    "# No negative monetary values\n",
    "assert (order_product_seller_agg_df[\"total_value\"] >= 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420a7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating at order_id level\n",
    "\n",
    "order_level_df = (\n",
    "    order_product_seller_agg_df\n",
    "    .groupby(\"order_id\", as_index=False)\n",
    "    .agg(\n",
    "        # counts\n",
    "        num_products=(\"product_id\", \"nunique\"),\n",
    "        num_sellers=(\"seller_id\", \"nunique\"),\n",
    "\n",
    "        # timestamps\n",
    "        order_purchase_timestamp=(\"order_purchase_timestamp\", \"first\"),\n",
    "        last_shipping_limit_date=(\"last_shipping_limit_date\", \"max\"),\n",
    "\n",
    "        # order size\n",
    "        avg_order_size=(\"order_item_cnt\", \"mean\"),\n",
    "\n",
    "        # price\n",
    "        tot_order_price=(\"price\", \"sum\"),\n",
    "        avg_order_price=(\"price\", \"mean\"),\n",
    "\n",
    "        # freight\n",
    "        tot_order_freight_value=(\"freight_value\", \"sum\"),\n",
    "        avg_order_freight_value=(\"freight_value\", \"mean\"),\n",
    "\n",
    "        # total\n",
    "        tot_order_value=(\"total_value\", \"sum\"),\n",
    "        avg_order_value=(\"total_value\", \"mean\"),\n",
    "\n",
    "        # product preferences\n",
    "        pref_prod_category=(\"product_category_name\", safe_mode),\n",
    "        avg_prod_length=(\"product_name_lenght\", \"mean\"),\n",
    "        avg_prod_desc_length=(\"product_description_lenght\", \"mean\"),\n",
    "        avg_prod_photo_qty=(\"product_photos_qty\", \"mean\"),\n",
    "        avg_prod_weight_g=(\"product_weight_g\", \"mean\"),\n",
    "        avg_prod_length_cm=(\"product_length_cm\", \"mean\"),\n",
    "        avg_prod_height_cm=(\"product_height_cm\", \"mean\"),\n",
    "        avg_prod_width_cm=(\"product_width_cm\", \"mean\"),\n",
    "        pref_prod_category_english=(\"product_category_name_english\", safe_mode),\n",
    "\n",
    "        # seller preferences\n",
    "        pref_seller_zip_code=(\"seller_zip_code_prefix\", safe_mode),\n",
    "        pref_seller_city=(\"seller_city\", safe_mode),\n",
    "        pref_seller_state=(\"seller_state\", safe_mode),\n",
    "        pref_seller_lat=(\"geolocation_lat\", safe_mode),\n",
    "        pref_seller_lng=(\"geolocation_lng\", safe_mode),\n",
    "        pref_seller_geolocation_city=(\"geolocation_city\", safe_mode),\n",
    "        pref_seller_geolocation_state=(\"geolocation_state\", safe_mode),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c32429",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_level_df = order_level_df.drop(columns=[\"days_since_last_shipped\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "966c98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_level_df.to_csv(\"../../Data/processed/intermediate_output_orders.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b3a6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "# One row per order\n",
    "assert order_level_df[\"order_id\"].is_unique\n",
    "\n",
    "# No negative monetary values\n",
    "assert (order_level_df[\"tot_order_value\"] >= 0).all()\n",
    "\n",
    "# Timestamp consistency\n",
    "assert order_level_df[\"order_purchase_timestamp\"].notna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0d4bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-2016 written.\n",
      "10-2016 written.\n",
      "11-2016 written.\n",
      "12-2016 written.\n",
      "01-2017 written.\n",
      "02-2017 written.\n",
      "03-2017 written.\n",
      "04-2017 written.\n",
      "05-2017 written.\n",
      "06-2017 written.\n",
      "07-2017 written.\n",
      "08-2017 written.\n",
      "09-2017 written.\n",
      "10-2017 written.\n",
      "11-2017 written.\n",
      "12-2017 written.\n",
      "01-2018 written.\n",
      "02-2018 written.\n",
      "03-2018 written.\n",
      "04-2018 written.\n",
      "05-2018 written.\n",
      "06-2018 written.\n",
      "07-2018 written.\n",
      "08-2018 written.\n",
      "09-2018 written.\n",
      "10-2018 written.\n",
      "ALL SNAPSHOTS COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# define the \"Fixed\" structure of your pivoted features\n",
    "ALL_STATUSES = ['approved', 'canceled', 'created', 'delivered', 'invoiced', 'processing', 'shipped', 'unavailable']\n",
    "ALL_PAYMENTS = ['boleto', 'credit_card', 'debit_card', 'not_defined', 'voucher']\n",
    "STATUS_METRICS = ['num_orders', 'tot_pymt_val', 'num_rev', 'num_products', 'avg_order_size', 'tot_order_price', \n",
    "'tot_order_freight_value', 'tot_order_value']\n",
    "\n",
    "def aggregate_payments(df):\n",
    "    \"\"\"Aggregates payments and ensures all payment types exist as columns.\"\"\"\n",
    "    base = df.groupby(\"order_id\", as_index=False).agg(\n",
    "        tot_pymt_sqntl=(\"payment_sequential\", \"nunique\"),\n",
    "        avg_pymt_instllmnt=(\"payment_installments\", \"mean\"),\n",
    "        tot_pymt_val=(\"payment_value\", \"sum\"),\n",
    "    )\n",
    "\n",
    "    pivot = df.pivot_table(\n",
    "        index=\"order_id\", \n",
    "        columns=\"payment_type\", \n",
    "        values=\"payment_value\", \n",
    "        aggfunc=\"sum\", \n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Reindex to force all payment columns to exist\n",
    "    pivot = pivot.reindex(columns=ALL_PAYMENTS, fill_value=0)\n",
    "    pivot.columns = [f\"pymt_{c}\" for c in pivot.columns]\n",
    "    \n",
    "    return base.merge(pivot.reset_index(), on=\"order_id\", how=\"left\")\n",
    "\n",
    "def aggregate_reviews(df, cutoff):\n",
    "    f = df[df[\"review_creation_date\"] <= cutoff]\n",
    "    if f.empty:\n",
    "        return pd.DataFrame(columns=[\"order_id\", \"num_rev\", \"avg_rev_score\", \n",
    "                                   \"avg_rev_title_length\", \"avg_rev_length\", \n",
    "                                   \"days_since_lst_rev_creation\"])\n",
    "    \n",
    "    g = f.groupby(\"order_id\").agg(\n",
    "        num_rev=(\"review_id\", \"count\"),\n",
    "        avg_rev_score=(\"review_score\", \"mean\"),\n",
    "        avg_rev_title_length=(\"review_title_len\", \"mean\"),\n",
    "        avg_rev_length=(\"review_msg_len\", \"mean\"),\n",
    "        last_rev_date=(\"review_creation_date\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    g[\"days_since_lst_rev_creation\"] = (cutoff - g[\"last_rev_date\"]).dt.days\n",
    "    return g.drop(columns=\"last_rev_date\")\n",
    "\n",
    "payments = pd.read_csv(\"../../Data/raw/olist_order_payments_dataset.csv\")\n",
    "reviews = pd.read_csv(\"../../Data/raw/olist_order_reviews_dataset.csv\", parse_dates=[\"review_creation_date\"])\n",
    "orders_df = pd.read_csv(\"../../Data/raw/olist_orders_dataset.csv\", parse_dates=[\"order_purchase_timestamp\", \"order_approved_at\", \n",
    "\"order_delivered_carrier_date\",\"order_delivered_customer_date\", \"order_estimated_delivery_date\"])\n",
    "\n",
    "# Pre-process Reviews\n",
    "reviews = reviews.merge(orders_df[[\"order_id\", \"order_purchase_timestamp\"]], on=\"order_id\", how=\"left\")\n",
    "reviews = reviews.sort_values(\"order_purchase_timestamp\").drop_duplicates(\"review_id\", keep=\"first\")\n",
    "reviews[\"review_title_len\"] = reviews[\"review_comment_title\"].fillna(\"\").str.len()\n",
    "reviews[\"review_msg_len\"] = reviews[\"review_comment_message\"].fillna(\"\").str.len()\n",
    "\n",
    "payments_agg_df = aggregate_payments(payments)\n",
    "\n",
    "START_YEAR, START_MONTH = 2016, 9\n",
    "END_YEAR, END_MONTH = 2018, 10\n",
    "num_months = ((END_YEAR - START_YEAR) * 12 + (END_MONTH - START_MONTH) + 1)\n",
    "output_root = Path(\"../../Data/processed/customer_snapshots\")\n",
    "output_root.mkdir(exist_ok=True)\n",
    "\n",
    "for run_seq in range(1, num_months + 1):\n",
    "    year = START_YEAR + (START_MONTH - 1 + run_seq - 1) // 12\n",
    "    month = (START_MONTH - 1 + run_seq - 1) % 12 + 1\n",
    "    cutoff = pd.Timestamp(year, month, 1) + pd.offsets.MonthEnd(0)\n",
    "    next_month = cutoff + pd.offsets.Day(1)\n",
    "\n",
    "    orders_f = orders_df[orders_df[\"order_purchase_timestamp\"] <= cutoff].copy()\n",
    "    if orders_f.empty: continue\n",
    "\n",
    "    reviews_agg = aggregate_reviews(reviews, cutoff)\n",
    "\n",
    "    # Master Join at Order Level\n",
    "    df = (\n",
    "        orders_f\n",
    "        .merge(payments_agg_df, on=\"order_id\", how=\"left\")\n",
    "        .merge(reviews_agg, on=\"order_id\", how=\"left\")\n",
    "        .merge(order_level_df, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Fix Timestamps\n",
    "    df = df.drop(columns=[\"order_purchase_timestamp_y\"], errors=\"ignore\")\n",
    "    df = df.rename(columns={\"order_purchase_timestamp_x\": \"order_purchase_timestamp\"})\n",
    "\n",
    "    # Time Features\n",
    "    df[\"days_since_last_shipped\"] = (next_month - df[\"last_shipping_limit_date\"]).dt.days\n",
    "    df[\"days_since_lst_order_purchased\"] = (next_month - df[\"order_purchase_timestamp\"]).dt.days\n",
    "    df[\"days_since_lst_order_approved\"] = (next_month - df[\"order_approved_at\"]).dt.days\n",
    "    df[\"days_since_lst_order_delivered_carrier\"] = (next_month - df[\"order_delivered_carrier_date\"]).dt.days\n",
    "    df[\"days_since_lst_order_delivered_cust\"] = (next_month - df[\"order_delivered_customer_date\"]).dt.days\n",
    "    df[\"days_since_lst_shipping_llimit_date\"] = df[\"days_since_last_shipped\"]\n",
    "\n",
    "    # 1. Customer Scalar Features\n",
    "    cust_scalar = df.groupby(\"customer_id\", as_index=False).agg(\n",
    "        num_orders=(\"order_id\", \"nunique\"),\n",
    "        tot_pymt_sqntl=(\"tot_pymt_sqntl\", \"sum\"),\n",
    "        avg_pymt_instllmnt=(\"avg_pymt_instllmnt\", \"mean\"),\n",
    "        tot_pymt_val=(\"tot_pymt_val\", \"sum\"),\n",
    "        **{f\"tot_pymt_{p}\": (f\"pymt_{p}\", \"sum\") for p in ALL_PAYMENTS},\n",
    "        num_rev=(\"num_rev\", \"sum\"),\n",
    "        avg_rev_score=(\"avg_rev_score\", \"mean\"),\n",
    "        avg_rev_title_length=(\"avg_rev_title_length\", \"mean\"),\n",
    "        avg_rev_length=(\"avg_rev_length\", \"mean\"),\n",
    "        days_since_lst_rev_creation=(\"days_since_lst_rev_creation\", \"min\"),\n",
    "        num_products=(\"num_products\", \"sum\"),\n",
    "        num_sellers=(\"num_sellers\", \"sum\"),\n",
    "        avg_order_size=(\"avg_order_size\", \"mean\"),\n",
    "        tot_order_price=(\"tot_order_price\", \"sum\"),\n",
    "        avg_order_price=(\"avg_order_price\", \"mean\"),\n",
    "        tot_order_freight_value=(\"tot_order_freight_value\", \"sum\"),\n",
    "        avg_order_freight_value=(\"avg_order_freight_value\", \"mean\"),\n",
    "        tot_order_value=(\"tot_order_value\", \"sum\"),\n",
    "        avg_order_value=(\"avg_order_value\", \"mean\"),\n",
    "        days_since_last_shipped=(\"days_since_last_shipped\", \"min\"),\n",
    "        days_since_lst_order_purchased=(\"days_since_lst_order_purchased\", \"min\"),\n",
    "        days_since_lst_order_approved=(\"days_since_lst_order_approved\", \"min\"),\n",
    "        days_since_lst_order_delivered_carrier=(\"days_since_lst_order_delivered_carrier\", \"min\"),\n",
    "        days_since_lst_order_delivered_cust=(\"days_since_lst_order_delivered_cust\", \"min\"),\n",
    "        days_since_lst_shipping_llimit_date=(\"days_since_lst_shipping_llimit_date\", \"min\"),\n",
    "        pref_prod_category=(\"pref_prod_category\", safe_mode),\n",
    "        pref_prod_category_english=(\"pref_prod_category_english\", safe_mode),\n",
    "    )\n",
    "\n",
    "    # 2. Consistent Status Pivot\n",
    "    status_base = df.groupby([\"customer_id\", \"order_status\"], as_index=False).agg(\n",
    "        num_orders=(\"order_id\", \"nunique\"),\n",
    "        tot_pymt_val=(\"tot_pymt_val\", \"sum\"),\n",
    "        num_rev=(\"num_rev\", \"sum\"),\n",
    "        num_products=(\"num_products\", \"sum\"),\n",
    "        avg_order_size=(\"avg_order_size\", \"mean\"),\n",
    "        tot_order_price=(\"tot_order_price\", \"sum\"),\n",
    "        tot_order_freight_value=(\"tot_order_freight_value\", \"sum\"),\n",
    "        tot_order_value=(\"tot_order_value\", \"sum\"),\n",
    "    )\n",
    "\n",
    "    status_pivot = status_base.pivot(index=\"customer_id\", columns=\"order_status\")\n",
    "    \n",
    "    # Enforce global column template for status pivot\n",
    "    full_columns = pd.MultiIndex.from_product([STATUS_METRICS, ALL_STATUSES])\n",
    "    status_pivot = status_pivot.reindex(columns=full_columns, fill_value=0)\n",
    "    status_pivot.columns = [f\"{metric}_{status}\" for metric, status in status_pivot.columns]\n",
    "    status_pivot = status_pivot.reset_index()\n",
    "\n",
    "    # 3. Join & Aggregate to Unique ID\n",
    "    customer_final = cust_scalar.merge(status_pivot, on=\"customer_id\", how=\"left\")\n",
    "    cust_joined = customers.merge(customer_final, on=\"customer_id\", how=\"left\")\n",
    "    cust_joined = cust_joined[cust_joined[\"num_orders\"].notna()].copy()\n",
    "\n",
    "    # Define aggregation logic for rolling up to unique_id\n",
    "    agg_dict = {\n",
    "        'customer_city': safe_mode, \n",
    "        'customer_state': safe_mode,\n",
    "        'customer_zip_code_prefix': 'first'\n",
    "    }\n",
    "    for col in cust_joined.columns:\n",
    "        if col in agg_dict or col in [\"customer_id\", \"customer_unique_id\"]: continue\n",
    "        if col.startswith(\"num_\") or col.startswith(\"tot_\"): agg_dict[col] = \"sum\"\n",
    "        elif col.startswith(\"days_since_\"): agg_dict[col] = \"min\"\n",
    "        elif col.startswith(\"avg_\"): agg_dict[col] = \"mean\"\n",
    "        elif col.startswith(\"pref_\"): agg_dict[col] = safe_mode\n",
    "\n",
    "    customer_unique_final = cust_joined.groupby(\"customer_unique_id\", as_index=False).agg(agg_dict)\n",
    "\n",
    "    # 4. Write Output\n",
    "    run_folder = output_root / f\"{month:02d}-{year}\"\n",
    "    run_folder.mkdir(exist_ok=True)\n",
    "    customer_unique_final.to_csv(run_folder / \"customer_unique_snapshot.csv\", index=False)\n",
    "    print(f\"{month:02d}-{year} written.\")\n",
    "\n",
    "print(\"ALL SNAPSHOTS COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473501a",
   "metadata": {},
   "source": [
    "### Customer snapshots have been written. The tables are at customer_unique_id and year-month level.\n",
    "\n",
    "Meaning that each table contains the details of customers till that point of itneraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
